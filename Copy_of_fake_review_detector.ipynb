{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sumit-rautela/Fake-Review-Detector/blob/main/Copy_of_fake_review_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WagRCpDiDn7T"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets scikit-learn torch\n",
        "!pip install lime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRrhrraTHcON"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiX_JnvNn1ov"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load specific columns and rename\n",
        "df_reduced = pd.read_csv(\"/content/fake reviews dataset.csv\")[['label', 'text_']]\n",
        "# Corrected typo: rename columns of df_reduced, not df\n",
        "df_reduced = df_reduced.rename(columns={'text_': 'text'})\n",
        "\n",
        "# Map labels and drop missing entries\n",
        "df_reduced['label'] = df_reduced['label'].map({'OR': 0, 'CG': 1})  # OR = original, CG = computer-generated\n",
        "df_reduced = df_reduced.dropna(subset=['label', 'text'])\n",
        "\n",
        "# Check if the DataFrame is empty after dropping NaNs\n",
        "print(f\"Shape of df_reduced after dropping NaNs: {df_reduced.shape}\")\n",
        "\n",
        "# Reduce to 6000 rows - this will only work if df_reduced has rows\n",
        "# If the shape printed above is (0, 2), the issue is with the data or mapping/dropping\n",
        "df = df_reduced.sample(n=6000, random_state=42)\n",
        "print(f\"Shape of df_reduced after dropping NaNs: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "117VFKEXh1Fo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "\n",
        "# Split the data\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize(texts):\n",
        "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "train_encodings = tokenize(train_texts)\n",
        "test_encodings = tokenize(test_texts)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = Dataset.from_dict({\n",
        "    'input_ids': train_encodings['input_ids'],\n",
        "    'attention_mask': train_encodings['attention_mask'],\n",
        "    'labels': train_labels  # Use \"labels\" key for Trainer\n",
        "})\n",
        "test_dataset = Dataset.from_dict({\n",
        "    'input_ids': test_encodings['input_ids'],\n",
        "    'attention_mask': test_encodings['attention_mask'],\n",
        "    'labels': test_labels\n",
        "})\n",
        "\n",
        "# Load the model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "# Define evaluation metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = torch.argmax(torch.tensor(logits), dim=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
        "\n",
        "# Define training arguments (compatible with older versions of transformers)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert_results\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_dir=\"./logs\",\n",
        "    # Remove or comment out problematic parameters\n",
        "    # save_strategy=\"no\",\n",
        "    # logging_strategy=\"epoch\",\n",
        "    # evaluation_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0bZ9IAizTrL"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQLCKmrOzV-v"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tY5D8IWT01Sl"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, HfFolder, Repository\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Set your model repo name (your-username/your-model-name)\n",
        "model_repo = \"lol3445/bert-email-cap-classifier\"\n",
        "\n",
        "# Save model and tokenizer locally first\n",
        "model.save_pretrained(\"saved_bert_model\")\n",
        "tokenizer.save_pretrained(\"saved_bert_model\")\n",
        "\n",
        "# Push to the Hub\n",
        "model.push_to_hub(model_repo)\n",
        "tokenizer.push_to_hub(model_repo)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXj9yTwSrxTB"
      },
      "outputs": [],
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation results: {eval_results}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H1shl8lSHbm"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHCSA-nJA71O"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoGNLD3d4XOk"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load model/tokenizer\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"lol3445/bert-email-cap-classifier\")\n",
        "    model = BertForSequenceClassification.from_pretrained(\"lol3445/bert-email-cap-classifier\")\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = load_model()\n",
        "\n",
        "def classify_email(text, tokenizer, model):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.softmax(outputs.logits, dim=1)\n",
        "        prediction = torch.argmax(probs, dim=1).item()\n",
        "        confidence = probs[0][prediction].item()\n",
        "    label = \"Original\" if prediction == 1 else \"Computer-Generated\"\n",
        "    return label, confidence\n",
        "\n",
        "def predict_proba(texts):\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.softmax(outputs.logits, dim=1)\n",
        "    return probs.cpu().numpy()\n",
        "\n",
        "explainer = LimeTextExplainer(class_names=[\"Computer-Generated\", \"Original\"])\n",
        "\n",
        "st.set_page_config(page_title=\"Fake Review Detector\", layout=\"centered\")\n",
        "st.title(\"🕵️‍♂️ Fake Review Detector\")\n",
        "st.write(\"Determine if a review is **Original** or **Computer-Generated** using BERT + LIME.\")\n",
        "\n",
        "user_input = st.text_area(\"✍️ Enter your Review Text:\")\n",
        "min_words_for_lime = 3\n",
        "\n",
        "# Define label and confidence up front\n",
        "label = None\n",
        "confidence = None\n",
        "\n",
        "if st.button(\"Classify\"):\n",
        "    if user_input.strip() == \"\":\n",
        "        st.warning(\"Please enter some text.\")\n",
        "    else:\n",
        "        label, confidence = classify_email(user_input, tokenizer, model)\n",
        "        st.success(f\"**Prediction: {label}**\")\n",
        "        st.info(f\"Confidence Score: {confidence:.2%}\")\n",
        "        st.progress(confidence)\n",
        "\n",
        "        # Feedback section\n",
        "        st.markdown(\"### Was this prediction correct?\")\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            if st.button(\"👍 Yes, correct\"):\n",
        "                st.success(\"✅ Thanks for your feedback!\")\n",
        "                with open(\"feedback_log.txt\", \"a\") as f:\n",
        "                    f.write(f\"👍 | Text: {user_input} | Predicted: {label} | Confidence: {confidence:.2%}\\n\")\n",
        "        with col2:\n",
        "            if st.button(\"👎 No, incorrect\"):\n",
        "                st.warning(\"⚠️ We'll use this to improve the model.\")\n",
        "                with open(\"feedback_log.txt\", \"a\") as f:\n",
        "                    f.write(f\"👎 | Text: {user_input} | Predicted: {label} | Confidence: {confidence:.2%}\\n\")\n",
        "\n",
        "# LIME Explanation\n",
        "show_explanation = st.checkbox(\n",
        "    \"🔍 Show Explanation (LIME)\",\n",
        "    disabled=(len(user_input.strip().split()) < min_words_for_lime)\n",
        ")\n",
        "\n",
        "if show_explanation and user_input.strip():\n",
        "    with st.spinner(\"Generating explanation...\"):\n",
        "        try:\n",
        "            explanation = explainer.explain_instance(\n",
        "                user_input,\n",
        "                predict_proba,\n",
        "                num_features=min(10, len(user_input.strip().split())),\n",
        "                num_samples=100\n",
        "            )\n",
        "            lime_html = explanation.as_html()\n",
        "            wrapped_html = f\"\"\"\n",
        "            <div style=\"background-color: white; color: black; padding: 20px; border-radius: 10px;\">\n",
        "                <style>\n",
        "                    .lime-explanation {{\n",
        "                        background-color: white !important;\n",
        "                        color: black !important;\n",
        "                    }}\n",
        "                    .lime-explanation table {{\n",
        "                        background-color: white !important;\n",
        "                        color: black !important;\n",
        "                    }}\n",
        "                    .lime-explanation td, .lime-explanation th {{\n",
        "                        background-color: white !important;\n",
        "                        color: black !important;\n",
        "                        border: 1px solid #ccc !important;\n",
        "                    }}\n",
        "                    .lime-explanation .positive {{\n",
        "                        background-color: #90EE90 !important;\n",
        "                        color: black !important;\n",
        "                    }}\n",
        "                    .lime-explanation .negative {{\n",
        "                        background-color: #FFB6C1 !important;\n",
        "                        color: black !important;\n",
        "                    }}\n",
        "                </style>\n",
        "                {lime_html}\n",
        "            </div>\n",
        "            \"\"\"\n",
        "            st.components.v1.html(wrapped_html, height=600, scrolling=True)\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Could not generate explanation: {str(e)}\")\n",
        "            st.info(\"💡 Try entering a longer, more descriptive review.\")\n",
        "\n",
        "# CSV Upload for Bulk Detection\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"📁 Upload a CSV file for Bulk Detection\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload a CSV file with a column named `text`\", type=[\"csv\"])\n",
        "\n",
        "if uploaded_file:\n",
        "    try:\n",
        "        df_upload = pd.read_csv(uploaded_file)\n",
        "\n",
        "        if \"text\" not in df_upload.columns:\n",
        "            st.error(\"❌ The CSV must contain a column named `text`.\")\n",
        "        else:\n",
        "            texts = df_upload[\"text\"].astype(str).tolist()\n",
        "            probs = predict_proba(texts)\n",
        "\n",
        "            preds = np.argmax(probs, axis=1)\n",
        "            confidences = probs[np.arange(len(preds)), preds]\n",
        "\n",
        "            results_df = pd.DataFrame({\n",
        "                \"text\": texts,\n",
        "                \"prediction\": [\"Original\" if p == 1 else \"Computer-Generated\" for p in preds],\n",
        "                \"confidence\": [f\"{c:.2%}\" for c in confidences]\n",
        "            })\n",
        "\n",
        "            st.success(f\"✅ Processed {len(results_df)} reviews.\")\n",
        "            st.dataframe(results_df)\n",
        "\n",
        "            csv_download = results_df.to_csv(index=False).encode(\"utf-8\")\n",
        "            st.download_button(\"⬇️ Download Results as CSV\", csv_download, \"results.csv\", \"text/csv\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"⚠️ Error reading file: {e}\")\n",
        "\n",
        "if len(user_input.strip().split()) < min_words_for_lime:\n",
        "    st.info(f\"💡 Enter at least {min_words_for_lime} words to enable LIME explanation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koWJrKoN30-o"
      },
      "outputs": [],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG1MKrqE39zo"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}